# 障害解析レポート: ffmpegプロセスリークによるカメラ接続全喪失

**検知日時**: 2026-01-05 07:50 JST（ユーザー報告）
**初回発生**: 2026-01-05 03:10 JST（最初のゾンビプロセス）
**継続状態**: 進行中（プロセス数増加継続）
**影響範囲**: is22全カメラ（特に192.168.125サブネット）
**重大度**: Critical

> **注記**: is22サーバーはUTC時刻で動作。本レポートではJST（UTC+9）に変換して記載。

---

## 1. 障害概要

is22のポーリング処理において、ffmpegプロセスがタイムアウト後も終了せずに蓄積し続け、Tapoカメラの接続制限（2-4接続）を超過。結果として大半のカメラへの接続が失敗する状態に陥った。

**障害は進行中**: 初回報告時25プロセス → 現在37プロセス（継続増加）

---

## 2. タイムライン

| UTC時刻 | JST時刻 | イベント |
|---------|---------|----------|
| Jan 4 16:23:14 | **Jan 5 01:23:14** | is22起動（PID 1468583） |
| Jan 4 18:10:04 | **Jan 5 03:10:04** | **最初のゾンビffmpeg発生**（192.168.125.12、PID 1504968） |
| Jan 4 21:07:42 | **Jan 5 06:07:42** | 192.168.125.19へのffmpegプロセス蓄積開始 |
| Jan 4 21:07〜22:24 | Jan 5 06:07〜07:24 | 192.168.125.19へのffmpegが22個蓄積（初回報告時） |
| Jan 4 22:24 | **Jan 5 07:24** | 初回解析時点: 合計25個 |
| Jan 4 22:51 | **Jan 5 07:51** | 追跡解析時点: 合計37個（増加継続中） |

**ゾンビプロセス蓄積パターン**:
- 起動後約2時間で最初のゾンビ発生
- 06:07 JST以降、約2分間隔で継続蓄積
- サービス再起動なしでは自然解消しない

---

## 3. 診断データ

### 3.1 ffmpegプロセス状況（最新: 07:51 JST）

```
=== ffmpegの接続先カメラ集計 ===
     32 192.168.125.19    ← Tapo C110 - 接続枯渇（22→32に増加）
      1 192.168.125.79    ← Tapo
      1 192.168.125.62    ← 接続中ハング
      1 192.168.125.45    ← 新規追加
      1 192.168.125.17    ← 新規追加
      1 192.168.125.12    ← 最古（6時間40分生存）

=== ffmpegプロセス数 ===
37（初回報告時25 → 現在37）
```

### 3.2 最古のプロセス

```
PID 1504968, started Sun Jan  4 18:10:04 2026 (UTC)
                   = Sun Jan  5 03:10:04 2026 (JST)
Parent: 1468583 (is22 camserver)
Target: 192.168.125.12:554
Running time: 6時間40分以上（継続中）
```

### 3.3 ネットワーク接続状態

```
=== RTSP接続状態（ポート554）===
     37 192.168.125.246 (is22サーバー) ← 25→37に増加
```

---

## 4. 根本原因分析

### 4.1 コード上の問題

**ファイル**: `src/snapshot_service/mod.rs` (270-288行)

```rust
async fn capture_rtsp(&self, rtsp_url: &str) -> Result<Vec<u8>> {
    let output = Command::new("ffmpeg")
        .args([...])
        .output()  // ← タイムアウト処理なし、子プロセス管理なし
        .await
        .map_err(...)?;
    ...
}
```

**ファイル**: `src/polling_orchestrator/mod.rs` (738-768行)

```rust
let capture_result = match tokio::time::timeout(
    Duration::from_millis(SNAPSHOT_TIMEOUT_MS),  // 30秒
    snapshot_service.capture(camera),
)
.await
{
    Ok(Ok(result)) => result,
    Err(_) => {
        // タイムアウト - しかしffmpegプロセスは生存
        return Err(...);
    }
}
```

### 4.2 問題の連鎖

1. ffmpegがRTSP接続でハング（カメラ無応答など）
2. 30秒タイムアウト発生 → Futureがキャンセル
3. **ffmpegプロセスは生存したまま**（RTSPコネクション保持）
4. 次のポーリングサイクルで新しいffmpegが起動
5. Tapoカメラは最大2-4接続制限 → 接続スロット枯渇
6. 新規リクエストは「Operation not permitted」で失敗
7. 全カメラへの接続が波及的に失敗

### 4.3 設計上の問題

**欠落している機能**:

1. **サブネット巡回終了時のクリーンアップ**: 実装されていない
2. **go2rtcストリーム解除**: `StreamGateway.remove_source()`は存在するが未使用
3. **ffmpegプロセスのライフサイクル管理**: RtspManagerは論理的ロックのみ
4. **コンストラクト-ディストラクト型巡回**: 設計意図どおり実装されていない

**期待される動作**（設計意図）:
- 巡回終了時にWebSocket/RTSP接続を切断
- クールダウン期間中はクリーンな状態
- 再生中はセッション継続
- 再生終了後に通常ループに戻る
- 次の巡回開始時に接続を再確立

---

## 5. RtspManager分析

**ファイル**: `src/rtsp_manager/mod.rs`

```rust
/// RTSPアクセスリース - Dropで自動解放
pub struct RtspLease {
    camera_id: String,
    _guard: tokio::sync::OwnedMutexGuard<()>,  // 論理的ロックのみ
}

impl Drop for RtspLease {
    fn drop(&mut self) {
        // ロック解放のみ、ffmpegプロセスは殺さない
        tracing::debug!(camera_id = %self.camera_id, "RTSP access released");
    }
}
```

**問題**: RtspLeaseがDropされてもffmpegプロセスは終了しない。

---

## 6. 修正方針

### 6.1 即時対応（短期）

1. `capture_rtsp`で`Command::spawn() + kill()`を使用
2. タイムアウト時に子プロセスを明示的にkill

```rust
async fn capture_rtsp(&self, rtsp_url: &str) -> Result<Vec<u8>> {
    let mut child = Command::new("ffmpeg")
        .args([...])
        .stdout(Stdio::piped())
        .stderr(Stdio::piped())
        .spawn()?;

    match tokio::time::timeout(
        Duration::from_secs(self.ffmpeg_timeout),
        child.wait_with_output()
    ).await {
        Ok(output) => { ... }
        Err(_) => {
            // タイムアウト - 子プロセスを明示的にkill
            let _ = child.kill().await;
            return Err(...);
        }
    }
}
```

### 6.2 中期対応

1. サブネット巡回終了時にgo2rtcストリーム解除を実装
2. RtspManagerにプロセス管理機能を追加
3. 定期的なゾンビプロセスチェック＆クリーンアップ

### 6.3 長期対応

1. go2rtc経由のスナップショット取得を優先（ffmpeg使用を減らす）
2. コンストラクト-ディストラクト型巡回の完全実装
3. プロセスリソース監視とアラート

---

## 7. 影響を受けたカメラ

| カメラIP | 機種 | 状態 | エラー |
|----------|------|------|--------|
| 192.168.125.19 | Tapo C110 | **重度** | 32プロセス占有、接続枯渇 |
| 192.168.125.12 | Tapo | 中度 | 最古ゾンビ（6時間40分生存） |
| 192.168.125.62 | - | 中度 | プロセスハング |
| 192.168.125.45 | - | 中度 | ゾンビ蓄積 |
| 192.168.125.17 | - | 中度 | ゾンビ蓄積（新規） |
| 192.168.125.79 | Tapo | 軽度 | 応答遅延 |
| その他全カメラ | - | 波及影響 | 接続スロット枯渇による接続失敗 |

**Tapoカメラの接続制限**: 2-4接続/台
→ 192.168.125.19は32接続で完全飽和

---

## 8. 再発防止策

1. **コードレビュー**: 子プロセス起動時は必ずkill処理を実装
2. **監視強化**: ffmpegプロセス数のアラート設定
3. **テスト追加**: タイムアウト時のプロセス終了確認テスト
4. **定期クリーンアップ**: 5分以上生存するffmpegプロセスを自動kill

---

## 9. go2rtc統合の問題点

### 9.1 現在の実装

```
go2rtc優先条件: producersのrecv > 0（=視聴者がいる場合のみ）
```

**現在の状態**（障害発生時）:
- go2rtc登録ストリーム: 17件
- 視聴者あり: **0件**
- アクティブproducer: **0件**
- スナップショット取得元: **すべてffmpeg**

### 9.2 設計意図との乖離

**期待される動作**（ユーザー設計意図）:
1. サブネット巡回終了時にWSを切断
2. コンストラクト-ディストラクト型でクリーンなプロセス巡回
3. 再生中はセッション継続
4. 再生終了後は通常ループに戻る
5. クールダウン後に接続再確立して巡回

**実際の動作**:
1. go2rtcは**ライブビュー再生専用**として実装
2. 視聴者がいない通常のポーリングは**常にffmpeg**を使用
3. 巡回終了時のクリーンアップ処理**なし**
4. ffmpegプロセス管理の問題は**go2rtc統合後も残っている**

### 9.3 推奨される修正

1. **ポーリング時もgo2rtcのframe.jpeg APIを使用**
   - 視聴者がいなくてもgo2rtcにストリームを登録
   - frame.jpeg APIでスナップショット取得（ffmpeg不要）
   - go2rtcが内部でRTSP接続を管理

2. **サブネット巡回終了時のクリーンアップ**
   - 巡回完了後にgo2rtcからストリーム削除
   - 次の巡回開始時に再登録

---

## 10. 関連ファイル

- `src/snapshot_service/mod.rs` - スナップショット取得（要修正）
- `src/polling_orchestrator/mod.rs` - ポーリング制御
- `src/rtsp_manager/mod.rs` - RTSPアクセス制御
- `src/stream_gateway/mod.rs` - go2rtc統合

---

## 11. タイムゾーン・環境情報

### 11.1 サーバー環境

```
Hostname: is22サーバー (192.168.125.246)
OS: Linux (OrangePi)
Timezone: Etc/UTC (UTC, +0000)
System clock synchronized: yes
```

### 11.2 クライアント環境

```
Timezone: Asia/Tokyo (JST, +0900)
```

### 11.3 時刻変換

ログ解析時は以下の変換が必要:
- **サーバーログ（UTC）→ JST**: +9時間
- 例: `Jan 4 18:10:04 UTC` = `Jan 5 03:10:04 JST`

---

## 12. 更新履歴

| 日時 (JST) | 内容 |
|------------|------|
| 2026-01-05 07:24 | 初回解析・レポート作成（25プロセス） |
| 2026-01-05 07:51 | 追跡解析（37プロセス、継続増加確認） |
| 2026-01-05 07:55 | タイムゾーン補正、タイムライン更新 |

---

**報告者**: Claude Code
**ステータス**: 解析完了・継続監視中、修正待ち
**GitHub Issue**: [#73](https://github.com/warusakudeveroper/aranea_ISMS/issues/73)
